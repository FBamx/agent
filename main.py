import asyncio
from llm import LLMClient
from mcp_client import MCPServer


async def main():
    llm = LLMClient(model="deepseek-chat", base_url="https://api.deepseek.com",
                    api_key="sk-b75a12605fdb4dd48d2b2e286425c953")

    llm.system_prompt = "ä½ æ˜¯ä¸€ä¸ªç”Ÿæ´»å°åŠ©æ‰‹"

    llm.add_mcp_server(MCPServer(
        id="weather",
        name="weather",
        command="uv",
        args=["run", "./mcp_server/server.py"]
    ))

    await llm.get_mcp_tools()

    print("AIåŠ©æ‰‹ğŸ¤–å·²å¯åŠ¨, è¾“å…¥ 'quit' é€€å‡º, 'history' æ˜¾ç¤ºå†å²æ¶ˆæ¯, 'clear' æ¸…ç©ºå†å²æ¶ˆæ¯")

    while True:
        user_input = input("\n\nç”¨æˆ·ğŸ˜Š: ").strip()
        if user_input.lower() == "quit":
            print("é€€å‡ºAIåŠ©æ‰‹")
            break
        elif user_input.lower() == "history":
            for i, history_message in enumerate(llm.conversation_history, 1):
                print(f"\nç¬¬{i}è½®å¯¹è¯: ")
                print(f"ç”¨æˆ·ğŸ˜Š: {history_message.user_content}")
                print(f"AIåŠ©æ‰‹ğŸ¤–: {history_message.ai_content}")
        elif user_input.lower() == "clear":
            llm.clear_conversation_history()
        else:
            print("\nAIåŠ©æ‰‹ğŸ¤–: ", end="")
            await llm.chat(user_input)


if __name__ == "__main__":
    asyncio.run(main())
